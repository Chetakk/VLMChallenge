{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d375db",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers peft bitsandbytes torch pytorch-cuda=12.1 opencv-python pillow\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72cc44a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Find your data\n",
    "data_path = Path(\"/kaggle/input\")\n",
    "print(\"Available input files:\")\n",
    "for item in sorted(data_path.rglob(\"*.mp4\"))[:5]:\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "# Load index\n",
    "index_file = None\n",
    "for f in data_path.rglob(\"index.json\"):\n",
    "    index_file = f\n",
    "    break\n",
    "\n",
    "if index_file:\n",
    "    with open(index_file) as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"\\nTraining samples: {data['total_samples']}\")\n",
    "    print(f\"Sample: {data['samples'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e9ab8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Qwen2_5VLForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "# 4-bit quantization config (CRITICAL for T4 VRAM)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(\"Loading Qwen2.5-VL-2B-Instruct...\")\n",
    "model = Qwen2_5VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-2B-Instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-2B-Instruct\")\n",
    "print(f\"âœ… Model loaded. Size: {model.get_memory_footprint() / 1e9:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65abeefe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Configure LoRA for fast fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRA rank (low rank = less params to tune)\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"âœ… LoRA adapter applied\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5874727d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class WarehouseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, processor, num_samples=20):\n",
    "        self.processor = processor\n",
    "        self.num_samples = num_samples\n",
    "        self.operations = [\"Box Setup\", \"Inner Packing\", \"Tape\", \"Put Items\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        op_idx = idx % len(self.operations)\n",
    "        instruction = (\n",
    "            f\"Analyze this warehouse packaging video. \"\n",
    "            f\"The main operation is {self.operations[op_idx]}. \"\n",
    "            f\"What operation happens next? Choose from: {', '.join(self.operations)}\"\n",
    "        )\n",
    "        response = f\"The next operation is: {self.operations[(op_idx + 1) % len(self.operations)]}\"\n",
    "        \n",
    "        # Combine instruction and response for training\n",
    "        text = f\"{instruction}\\n{response}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenized = self.processor.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": tokenized[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": tokenized[\"input_ids\"].squeeze().clone(),\n",
    "        }\n",
    "\n",
    "dataset = WarehouseDataset(processor, num_samples=20)\n",
    "print(f\"âœ… Dataset created: {len(dataset)} samples\")\n",
    "sample = dataset[0]\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(f\"Input shape: {sample['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1451381e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DefaultDataCollator\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/checkpoint\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    report_to=[],  # Disable wandb\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=DefaultDataCollator(),\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Starting fine-tuning (8-10 hours)...\")\n",
    "trainer.train()\n",
    "print(\"âœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef34fefe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"/kaggle/working/qwen-lora-checkpoint\")\n",
    "processor.save_pretrained(\"/kaggle/working/qwen-lora-checkpoint\")\n",
    "\n",
    "print(\"âœ… Checkpoint saved to /kaggle/working/qwen-lora-checkpoint\")\n",
    "print(\"\\nDownload these files from the Output panel:\")\n",
    "print(\"  - qwen-lora-checkpoint/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
